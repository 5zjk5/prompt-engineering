# LangChain 测试示例

本目录包含了各种 LangChain 测试和评估的代码示例，展示了如何测试和验证 LangChain 应用程序的不同方面。

## 文件列表

### 1. agentevals-strict.py
**功能**: 演示如何使用严格模式评估智能体轨迹

该示例展示了如何使用 `trajectory_match_mode="strict"` 来确保智能体按照特定顺序调用工具。严格模式验证轨迹中包含相同顺序且工具调用相同的消息，但允许消息内容存在差异。

**主要特点**:
- 创建一个简单的智能体，使用 `get_weather` 工具
- 设置严格评估器，验证工具调用顺序
- 演示了正确和错误的参考轨迹对比

**适用场景**: 当需要强制执行特定操作顺序时，例如在授权操作之前要求进行策略查找。

### 2. agentevals-other.py（不可用）
**功能**: 演示如何使用其他模式评估智能体轨迹

该示例展示了如何使用 `trajectory_match_mode="subset"` 来评估智能体轨迹。子集模式确保智能体没有调用参考轨迹之外的任何工具。

**主要特点**:
- 创建一个智能体，使用多个工具（`get_weather`, `get_events`, `get_name`）
- 设置子集评估器，验证工具调用
- 演示了如何评估多个工具的调用情况

**适用场景**: 当需要验证智能体是否只调用了预期的工具，但没有调用其他工具时。

### 3. inmemorysaver_checkpointer.py
**功能**: 演示如何使用内存检查点保存对话历史

该示例展示了如何使用 `InMemorySaver` 来保存对话历史，使智能体能够记住之前的交互内容。

**主要特点**:
- 创建一个带有内存检查点的智能体
- 使用 `RunnableConfig` 配置线程ID
- 演示了如何保持对话上下文

**适用场景**: 当需要智能体记住之前的对话内容，以便进行连续对话时。

### 4. llm_as_judge_evaluator.py
**功能**: 演示如何使用LLM作为评估器来评估智能体轨迹

该示例展示了如何使用 `create_trajectory_llm_as_judge` 函数，让LLM评估智能体的执行路径。与轨迹匹配评估器不同，它不需要参考轨迹，但如果存在参考轨迹，也可以提供。

**主要特点**:
- 创建一个智能体，使用 `get_weather` 工具
- 设置LLM评估器，使用预定义的提示词
- 演示了如何评估智能体轨迹的准确性

**适用场景**: 当需要更灵活的评估方式，可以根据场景自定义提示词来评估智能体的执行路径时。

### 5. mocking_chat_model.py
**功能**: 演示如何模拟聊天模型

该示例展示了如何使用 `GenericFakeChatModel` 来模拟聊天模型的行为，这对于测试和开发非常有用。

**主要特点**:
- 创建一个模拟聊天模型，预定义响应消息
- 演示了如何模拟工具调用和普通文本响应
- 展示了如何使用模拟模型进行测试

**适用场景**: 当需要在没有实际LLM的情况下测试智能体行为，或者需要模拟特定的响应时。

